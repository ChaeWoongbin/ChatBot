import os
import streamlit as st
from dotenv import load_dotenv
import google.generativeai as genai
import pymupdf  # PyMuPDF
from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb # faiss ëŒ€ì‹  chromadb ì„í¬íŠ¸
import shutil # shutil ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('api_key.env')

# --- ì„¤ì • ë° ì´ˆê¸°í™” ---

# Google API í‚¤ ì„¤ì •
try:
    if "GEMINI_API_KEY" not in os.environ:
        st.error("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”! (.env íŒŒì¼)")
        st.stop()
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()

# --- ChromaDB ë²„ì „: docs í´ë”ì—ì„œ PDF ë¡œë“œ ë° ì²˜ë¦¬ ---

@st.cache_resource
def load_and_process_pdfs_from_docs():
    """'./docs' í´ë”ì—ì„œ ëª¨ë“  PDFë¥¼ ë¡œë“œí•˜ê³  ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤."""
    DOCS_PATH = "./docs"
    CHROMA_PATH = "./chroma_db" # ChromaDB ë°ì´í„°ë¥¼ ì €ì¥í•  í´ë”
    COLLECTION_NAME = "pdf_rag_collection"

    if not os.path.exists(DOCS_PATH):
        st.error(f"'{DOCS_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        st.stop()

    # ê¸°ì¡´ ChromaDB í´ë”ê°€ ìˆìœ¼ë©´ ì‚­ì œ
    if os.path.exists(CHROMA_PATH):
        try:
            shutil.rmtree(CHROMA_PATH)
            # print(f"ê¸°ì¡´ ChromaDB í´ë” '{CHROMA_PATH}' ì‚­ì œ ì™„ë£Œ.")
        except Exception as e:
            st.warning(f"ChromaDB í´ë” ì‚­ì œ ì‹¤íŒ¨: {e}. ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œí•˜ê±°ë‚˜ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            # ì‚­ì œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.

    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = chromadb.PersistentClient(path=CHROMA_PATH)

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
    collection = client.get_or_create_collection(name=COLLECTION_NAME)
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆë‹¤ë©´ ì‚­ì œ (í•­ìƒ ìµœì‹  docs í´ë” ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ ë§Œë“¤ê¸° ìœ„í•¨)
    # ë” ë°œì „ëœ ë°©ë²•: íŒŒì¼ ë³€ê²½ ì—¬ë¶€ë¥¼ ì²´í¬í•˜ì—¬ í•„ìš”í•  ë•Œë§Œ ì—…ë°ì´íŠ¸
    if COLLECTION_NAME in [c.name for c in client.list_collections()]:
        client.delete_collection(name=COLLECTION_NAME)

    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
    collection = client.create_collection(name=COLLECTION_NAME)
    
    all_text = ""
    loaded_files = []
    
    with st.spinner("`docs` í´ë”ì—ì„œ PDF íŒŒì¼ì„ ì½ëŠ” ì¤‘..."):
        for filename in os.listdir(DOCS_PATH):
            if filename.endswith(".pdf"):
                file_path = os.path.join(DOCS_PATH, filename)
                try:
                    with pymupdf.open(file_path) as doc:
                        text = ""
                        for page in doc:
                            text += page.get_text()
                        all_text += text + "\n\n"
                        loaded_files.append(filename)
                except Exception as e:
                    st.warning(f"'{filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    if not loaded_files:
        st.error("`docs` í´ë”ì— ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # í…ìŠ¤íŠ¸ ì²­í‚¹
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300,separators=[
                                                                                                    "\n\n",   # ë‹¨ë½
                                                                                                    "\n",     # ì¤„ë°”ê¿ˆ
                                                                                                    ".",      # ë§ˆì¹¨í‘œ (ë¬¸ì¥)
                                                                                                    "?",      # ë¬¼ìŒí‘œ
                                                                                                    "!",      # ëŠë‚Œí‘œ
                                                                                                    " ",      # ê³µë°± (ë‹¨ì–´)
                                                                                                    "",       # ë¬¸ì
                                                                                                ])
    chunks = text_splitter.split_text(all_text)

     # =========================================================
    # ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€
    print("="*50)
    print(f"ë¡œë“œëœ íŒŒì¼ ê°œìˆ˜: {len(loaded_files)}")
    print(f"ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(all_text)}")
    print(f"ìƒì„±ëœ ì²­í¬ ê°œìˆ˜: {len(chunks)}")
    print("="*50)
    # =========================================================

    # ì²­í¬ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ì–´ ì½”ë“œ ì¶”ê°€
    if not chunks:
        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PDFê°€ ì´ë¯¸ì§€ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop() # í”„ë¡œê·¸ë¨ ì¤‘ë‹¨


    # ì„ë² ë”© ë° ChromaDBì— ì €ì¥
    try:
        with st.spinner("ë¬¸ì„œ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ ì¤‘..."):
            result = genai.embed_content(
                model="models/embedding-001",
                content=chunks,
                task_type="retrieval_document"
            )
            embeddings = result['embedding']
            
            # ChromaDBì— ë°ì´í„° ì¶”ê°€
            collection.add(
                embeddings=embeddings,
                documents=chunks,
                ids=[f"chunk_{i}" for i in range(len(chunks))]
            )
            
            st.success("ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return collection, loaded_files
    except Exception as e:
        st.error(f"ì„ë² ë”© ë˜ëŠ” ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

# --- ChromaDB ë²„ì „: RAG ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_relevant_chunks(query, collection):
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì²­í¬ë¥¼ ChromaDBì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        query_embedding = genai.embed_content(
            model="models/embedding-001",
            content=query,
            task_type="retrieval_query"
        )['embedding']
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=6 # ìƒìœ„ 6ê°œ ê²°ê³¼
        )
        
        return results['documents'][0]
    except Exception as e:
        st.error(f"ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# --- Streamlit UI ---

st.set_page_config(page_title="ë¬¸ì„œ ê¸°ë°˜ Gemini ì±—ë´‡ (ChromaDB)", layout="centered")
st.title("ë¬¸ì„œ ê¸°ë°˜ Gemini ì±—ë´‡ ğŸ“š (ChromaDB)")

# ì•± ì‹œì‘ ì‹œ PDF ì²˜ë¦¬ ì‹¤í–‰
collection, loaded_files = load_and_process_pdfs_from_docs()

with st.sidebar:
    st.header("ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡")
    if loaded_files:
        for filename in loaded_files:
            st.info(filename)
    else:
        st.warning("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

MODEL_NAME = "gemini-2.5-flash"

if "messages" not in st.session_state:
    initial_message = f"`docs` í´ë”ì˜ {len(loaded_files)}ê°œ ë¬¸ì„œì— ëŒ€í•œ ë‚´ìš©ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•˜ì„¸ìš”."
    st.session_state.messages = [{"role": "assistant", "content": initial_message}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    st.empty() 
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
           #  message_placeholder = st.empty()
            
            relevant_chunks = get_relevant_chunks(prompt, collection)
            context = "\n\n".join(relevant_chunks)
            
            rag_prompt = f"""
            ë‹¹ì‹ ì€ ì „ì‚°ê´€ë¦¬ê·œì • ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œí„´íŠ¸ì…ë‹ˆë‹¤.
            ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ "ë¬¸ì„œ ë‚´ìš©"ì„ ê·¼ê±°ë¡œ í•´ì„œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
            ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€í•˜ì§€ ë§ê³ , "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.        
            í•˜ì§€ë§Œ, ë¬¸ì„œ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ì ì¸ ìƒì‹ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            ë‹µë³€ í›„ ë‹µë³€ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ê°„ëµíˆ ìš”ì•½í•´ì„œ "ì°¸ê³  ë¬¸ì„œ"ë¡œ ì œê³µí•˜ì„¸ìš”.
            ---
            [ë¬¸ì„œ ë‚´ìš©]
            {context}
            ---

            [ì§ˆë¬¸]
            {prompt}
            """
            
            try:
                model = genai.GenerativeModel(MODEL_NAME)
                response = model.generate_content(rag_prompt)
                full_response = response.text
                
                # message_placeholder.markdown(full_response) <-- ì´ ë¶€ë¶„ì„ ì‚­ì œí•©ë‹ˆë‹¤.
                
                # ìƒì„±ëœ ë‹µë³€ì„ Streamlit Chat UIì— í‘œì‹œ
                st.markdown(full_response) 
                
                # ë‹µë³€ ì™„ë£Œ í›„ session_stateì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            except Exception as e:
                error_message = f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ Streamlit Chat UIì— í‘œì‹œ
                st.error(error_message) 
                st.session_state.messages.append({"role": "assistant", "content": error_message})